nohup: ignoring input
run sh: `/usr/bin/python3.10 /usr/local/lib/python3.10/dist-packages/swift/cli/sft.py --model /root/paddlejob/workspace/env_run/output/VIDA/Qwen3-VL-8B-Instruct --dataset ../datasets/MS-S-train_sft.jsonl --output_dir output/qwen3_vl_8b_lora_sft_stable --train_type lora --lora_rank 64 --lora_alpha 32 --lora_dropout 0.05 --target_modules all-linear --freeze_vit true --torch_dtype bfloat16 --num_train_epochs 3 --learning_rate 1e-4 --warmup_ratio 0.03 --per_device_train_batch_size 8 --gradient_accumulation_steps 8 --max_length 2048 --max_pixels 1003520 --gradient_checkpointing true --save_steps 50 --eval_steps 50 --save_only_model true --save_total_limit 3 --logging_steps 5 --lazy_tokenize true --dataloader_num_workers 1`
[INFO:swift] Successfully registered `/usr/local/lib/python3.10/dist-packages/swift/llm/dataset/data/dataset_info.json`.
[INFO:swift] rank: -1, local_rank: -1, world_size: 1, local_world_size: 1
`torch_dtype` is deprecated! Use `dtype` instead!
[INFO:swift] output_dir: /root/paddlejob/workspace/env_run/output/VIDA/sft/output/qwen3_vl_8b_lora_sft_stable/v1-20251228-101733
[INFO:swift] Global seed set to 42
[INFO:swift] args: TrainArguments(
_n_gpu=-1,
acc_strategy=token,
accelerator_config={'dispatch_batches': False},
adafactor=False,
adalora_beta1=0.85,
adalora_beta2=0.85,
adalora_deltaT=1,
adalora_init_r=12,
adalora_orth_reg_weight=0.5,
adalora_target_r=8,
adalora_tfinal=0,
adalora_tinit=0,
adam_beta1=0.9,
adam_beta2=0.95,
adam_epsilon=1e-08,
adapter_act=gelu,
adapter_length=128,
adapters=[],
add_version=True,
agent_template=None,
aligner_lr=None,
attn_impl=None,
auto_find_batch_size=False,
average_tokens_across_devices=True,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
bnb_4bit_compute_dtype=torch.bfloat16,
bnb_4bit_quant_storage=None,
bnb_4bit_quant_type=nf4,
bnb_4bit_use_double_quant=True,
boft_block_num=0,
boft_block_size=4,
boft_dropout=0.0,
boft_n_butterfly_factor=1,
cached_dataset=[],
cached_val_dataset=[],
check_model=True,
ckpt_dir=None,
columns={},
create_checkpoint_symlink=False,
custom_dataset_info=[],
custom_register_path=[],
data_seed=42,
dataloader_drop_last=False,
dataloader_num_workers=1,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
dataset=['../datasets/MS-S-train_sft.jsonl'],
dataset_num_proc=1,
dataset_shuffle=True,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=18000000,
debug=None,
deepspeed=None,
deepspeed_autotp_size=None,
device_groups=None,
device_map=None,
disable_tqdm=None,
do_eval=False,
do_predict=False,
do_train=False,
download_mode=reuse_dataset_if_exists,
ds3_gather_for_generation=True,
early_stop_interval=None,
enable_channel_loss=False,
enable_dft_loss=False,
eval_accumulation_steps=None,
eval_dataset=[],
eval_dataset_args=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_generation_config=None,
eval_limit=None,
eval_on_start=False,
eval_steps=50.0,
eval_strategy=no,
eval_use_evalscope=False,
eval_use_gather_object=False,
external_plugins=[],
extra_eval_args=None,
fourier_n_frequency=2000,
fourier_scaling=300.0,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
freeze_aligner=True,
freeze_llm=False,
freeze_parameters=[],
freeze_parameters_ratio=0.0,
freeze_parameters_regex=None,
freeze_vit=True,
fsdp=None,
fsdp_config=None,
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
galore_cos_threshold=0.4,
galore_gamma_proj=2,
galore_optim_per_parameter=False,
galore_proj_bits=4,
galore_proj_group_size=256,
galore_proj_quant=False,
galore_proj_type=std,
galore_quantization=False,
galore_queue_size=5,
galore_rank=128,
galore_scale=1.0,
galore_target_modules=None,
galore_update_proj_gap=50,
galore_with_embedding=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=8,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=False,
group_by_length=False,
half_precision_backend=auto,
hqq_axis=None,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_revision=None,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_args_error=False,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
init_strategy=None,
init_weights=True,
interleave_prob=None,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
lazy_tokenize=True,
learning_rate=0.0001,
length_column_name=length,
liger_kernel_config=None,
lisa_activated_layers=0,
lisa_step_interval=20,
llamapro_num_groups=None,
llamapro_num_new_blocks=4,
load_args=False,
load_best_model_at_end=False,
load_data_args=False,
load_from_cache_file=False,
local_rank=-1,
local_repo_path=None,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/root/paddlejob/workspace/env_run/output/VIDA/sft/output/qwen3_vl_8b_lora_sft_stable/v1-20251228-101733/runs,
logging_first_step=True,
logging_nan_inf_filter=True,
logging_steps=5,
logging_strategy=steps,
logprobs=False,
lora_alpha=32,
lora_bias=none,
lora_dropout=0.05,
lora_dtype=None,
lora_ga_batch_size=2,
lora_ga_direction=ArB2r,
lora_ga_iters=2,
lora_ga_max_length=1024,
lora_ga_scale=stable,
lora_ga_stable_gamma=16,
lora_modules=[],
lora_rank=64,
lorap_lr_ratio=None,
loss_scale=default,
loss_type=None,
lr_scheduler_kwargs=None,
lr_scheduler_type=cosine,
max_epochs=None,
max_grad_norm=1.0,
max_length=2048,
max_memory={},
max_model_len=None,
max_new_tokens=64,
max_pixels=1003520,
max_steps=-1,
metric=None,
metric_for_best_model=loss,
model=/root/paddlejob/workspace/env_run/output/VIDA/Qwen3-VL-8B-Instruct,
model_author=None,
model_kwargs={},
model_name=None,
model_revision=None,
model_type=qwen3_vl,
modules_to_save=[],
mp_parameters=,
neftune_noise_alpha=None,
new_special_tokens=[],
no_cuda=False,
norm_bbox=None,
num_beams=1,
num_labels=None,
num_train_epochs=3.0,
optim=adamw_torch_fused,
optim_args=None,
optim_target_modules=None,
optimizer=None,
output_dir=/root/paddlejob/workspace/env_run/output/VIDA/sft/output/qwen3_vl_8b_lora_sft_stable/v1-20251228-101733,
overwrite_output_dir=False,
packing=False,
packing_length=None,
packing_num_proc=1,
padding_free=False,
padding_side=right,
parallelism_config=None,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=8,
predict_with_generate=False,
prediction_loss_only=False,
problem_type=None,
project=huggingface,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
quant_bits=None,
quant_method=None,
ray_exp_name=None,
ray_scope=last,
reft_args=None,
reft_intervention_type=LoreftIntervention,
reft_layer_key=None,
reft_layers=None,
reft_rank=4,
remove_unused_columns=True,
repetition_penalty=None,
report_to=['tensorboard'],
response_prefix=None,
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
resume_only_model=False,
rope_scaling=None,
router_aux_loss_coef=0.0,
run_name=/root/paddlejob/workspace/env_run/output/VIDA/sft/output/qwen3_vl_8b_lora_sft_stable/v1-20251228-101733,
save_on_each_node=False,
save_only_model=True,
save_safetensors=True,
save_steps=50.0,
save_strategy=steps,
save_total_limit=3,
seed=42,
sequence_parallel_size=1,
shuffle_buffer_size=1000,
skip_memory_metrics=True,
sortish_sampler=False,
split_dataset_ratio=0.0,
stop_words=[],
stopping_strategy=first_exhausted,
stream=False,
streaming=False,
strict=False,
swanlab_exp_name=None,
swanlab_lark_secret=None,
swanlab_lark_webhook_url=None,
swanlab_mode=cloud,
swanlab_project=None,
swanlab_token=<SWANLAB_TOKEN>,
swanlab_workspace=None,
system=None,
target_modules=['all-linear'],
target_parameters=None,
target_regex=None,
task_type=causal_lm,
temperature=0.0,
template=qwen3_vl,
template_backend=swift,
tf32=None,
top_k=None,
top_logprobs=None,
top_p=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_dtype=torch.bfloat16,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
trackio_space_id=trackio,
train_dataloader_shuffle=True,
train_type=lora,
trainable_parameters=[],
trainable_parameters_regex=None,
truncation_strategy=delete,
tuner_backend=peft,
use_chat_template=True,
use_cpu=False,
use_dora=False,
use_flash_ckpt=False,
use_galore=False,
use_hf=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_logits_to_keep=None,
use_mps_device=False,
use_ray=False,
use_rslora=False,
use_swift_lora=False,
val_dataset=[],
val_dataset_shuffle=False,
vera_d_initial=0.1,
vera_dropout=0.0,
vera_projection_prng_key=0,
vera_rank=256,
vit_gradient_checkpointing=None,
vit_lr=None,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.1,
zero_hpz_partition_size=None,
)
[INFO:swift] model_kwargs: {'device_map': 'cuda:0', 'dtype': torch.bfloat16}
Loading checkpoint shards:   0%|                                                            | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|█████████████                                       | 1/4 [00:01<00:05,  1.83s/it]Loading checkpoint shards:  50%|██████████████████████████                          | 2/4 [00:03<00:03,  1.87s/it]Loading checkpoint shards:  75%|███████████████████████████████████████             | 3/4 [00:05<00:01,  1.89s/it]Loading checkpoint shards: 100%|████████████████████████████████████████████████████| 4/4 [00:06<00:00,  1.59s/it]Loading checkpoint shards: 100%|████████████████████████████████████████████████████| 4/4 [00:06<00:00,  1.69s/it]
[INFO:swift] Setting max_ratio: 200. You can adjust this hyperparameter through the environment variable: `MAX_RATIO`.
[INFO:swift] Setting frame_factor: 2. You can adjust this hyperparameter through the environment variable: `FRAME_FACTOR`.
[INFO:swift] Setting fps: 2.0. You can adjust this hyperparameter through the environment variable: `FPS`.
[INFO:swift] Setting fps_min_frames: 4. You can adjust this hyperparameter through the environment variable: `FPS_MIN_FRAMES`.
[INFO:swift] Setting fps_max_frames: 768. You can adjust this hyperparameter through the environment variable: `FPS_MAX_FRAMES`.
[INFO:swift] Setting image_max_token_num: 16384. You can adjust this hyperparameter through the environment variable: `IMAGE_MAX_TOKEN_NUM`.
[INFO:swift] Setting image_min_token_num: 4. You can adjust this hyperparameter through the environment variable: `IMAGE_MIN_TOKEN_NUM`.
[INFO:swift] Setting spatial_merge_size: 2. You can adjust this hyperparameter through the environment variable: `SPATIAL_MERGE_SIZE`.
[INFO:swift] Setting video_max_token_num: 768. You can adjust this hyperparameter through the environment variable: `VIDEO_MAX_TOKEN_NUM`.
[INFO:swift] Setting video_min_token_num: 128. You can adjust this hyperparameter through the environment variable: `VIDEO_MIN_TOKEN_NUM`.
[INFO:swift] model.hf_device_map: {'': device(type='cuda', index=0)}
[INFO:swift] model_info: ModelInfo(model_type='qwen3_vl', model_dir='/root/paddlejob/workspace/env_run/output/VIDA/Qwen3-VL-8B-Instruct', torch_dtype=torch.bfloat16, max_model_len=262144, quant_method=None, quant_bits=None, rope_scaling={'mrope_interleaved': True, 'mrope_section': [24, 20, 20], 'rope_type': 'default'}, is_moe_model=False, is_multimodal=True, config=Qwen3VLConfig {
  "architectures": [
    "Qwen3VLForConditionalGeneration"
  ],
  "dtype": "bfloat16",
  "hidden_size": 4096,
  "image_token_id": 151655,
  "model_type": "qwen3_vl",
  "pad_token_id": 151643,
  "text_config": {
    "attention_bias": false,
    "attention_dropout": 0.0,
    "bos_token_id": 151643,
    "dtype": "bfloat16",
    "eos_token_id": 151645,
    "head_dim": 128,
    "hidden_act": "silu",
    "hidden_size": 4096,
    "initializer_range": 0.02,
    "intermediate_size": 12288,
    "max_position_embeddings": 262144,
    "model_type": "qwen3_vl_text",
    "num_attention_heads": 32,
    "num_hidden_layers": 36,
    "num_key_value_heads": 8,
    "pad_token_id": 151643,
    "rms_norm_eps": 1e-06,
    "rope_scaling": {
      "mrope_interleaved": true,
      "mrope_section": [
        24,
        20,
        20
      ],
      "rope_type": "default"
    },
    "rope_theta": 5000000,
    "use_cache": true,
    "vocab_size": 151936
  },
  "tie_word_embeddings": false,
  "transformers_version": "4.57.3",
  "video_token_id": 151656,
  "vision_config": {
    "deepstack_visual_indexes": [
      8,
      16,
      24
    ],
    "depth": 27,
    "dtype": "bfloat16",
    "hidden_act": "gelu_pytorch_tanh",
    "hidden_size": 1152,
    "in_channels": 3,
    "initializer_range": 0.02,
    "intermediate_size": 4304,
    "model_type": "qwen3_vl",
    "num_heads": 16,
    "num_position_embeddings": 2304,
    "out_hidden_size": 4096,
    "pad_token_id": 151643,
    "patch_size": 16,
    "spatial_merge_size": 2,
    "temporal_patch_size": 2
  },
  "vision_end_token_id": 151653,
  "vision_start_token_id": 151652
}
, task_type='causal_lm', num_labels=None)
[INFO:swift] model.generation_config: GenerationConfig {
  "bos_token_id": 151643,
  "eos_token_id": [
    151645,
    151643
  ],
  "max_new_tokens": 64,
  "pad_token_id": 151643
}

[INFO:swift] default_system: None
[INFO:swift] max_length: 2048
[INFO:swift] response_prefix: ''
[INFO:swift] agent_template: hermes
[INFO:swift] norm_bbox: norm1000
[INFO:swift] Setting ROOT_IMAGE_DIR: None. You can adjust this hyperparameter through the environment variable: `ROOT_IMAGE_DIR`.
[INFO:swift] Setting QWENVL_BBOX_FORMAT: legacy. You can adjust this hyperparameter through the environment variable: `QWENVL_BBOX_FORMAT`.
[INFO:swift] Start time of running main: 2025-12-28 10:17:41.344451
[INFO:swift] swift.__version__: 3.11.2
Map:   0%|                                                                        | 0/3042 [00:00<?, ? examples/s]Map:  33%|███████████████████▍                                       | 1000/3042 [00:00<00:00, 3954.91 examples/s]Map: 100%|██████████████████████████████████████████████████████████| 3042/3042 [00:00<00:00, 10579.94 examples/s]
[INFO:swift] train_dataset: Dataset({
    features: ['images', 'messages'],
    num_rows: 3042
})
[INFO:swift] val_dataset: None
[INFO:swift] [INPUT_IDS] [151644, 8948, 198, 2610, 525, 264, 6584, 14791, 14692, 13, 64547, 279, 1196, 594, 17995, 3880, 3118, 389, 279, 2168, 323, 1681, 13, 151645, 198, 151644, 872, 198, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 4340, 311, 2297, 432, 11, 432, 594, 49431, 323, 4287, 11, 42903, 1128, 3093, 315, 20704, 18824, 311, 14678, 13, 151645, 198, 151644, 77091, 198, 6713, 7703, 264, 10799, 1965, 22549, 6005, 1388, 35592, 14, 7119, 65926, 14, 484, 10692, 10779, 65659, 6862, 287, 30418, 279, 3550, 16566, 30, 151645, 198]
[INFO:swift] [INPUT] <|im_start|>system
You are a professional interior designer. Identify the user's implicit needs based on the image and request.<|im_end|>
<|im_start|>user
<|vision_start|>[151655 * 961]<|vision_end|>How to change it, it's bland and empty, unsure what kind of hanging painting to hang.<|im_end|>
<|im_start|>assistant
Can increasing a coffee table/decorative paintings/throw pillows/indoor plants/carpeting enrich the space atmosphere?<|im_end|>

[INFO:swift] [LABELS_IDS] [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 6713, 7703, 264, 10799, 1965, 22549, 6005, 1388, 35592, 14, 7119, 65926, 14, 484, 10692, 10779, 65659, 6862, 287, 30418, 279, 3550, 16566, 30, 151645, 198]
[INFO:swift] [LABELS] [-100 * 1016]Can increasing a coffee table/decorative paintings/throw pillows/indoor plants/carpeting enrich the space atmosphere?<|im_end|>

[INFO:swift] The TrainArguments will be saved in: /root/paddlejob/workspace/env_run/output/VIDA/sft/output/qwen3_vl_8b_lora_sft_stable/v1-20251228-101733/args.json
[INFO:swift] lora_config: LoraConfig(task_type='CAUSAL_LM', peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, peft_version='0.18.0', base_model_name_or_path='/root/paddlejob/workspace/env_run/output/VIDA/Qwen3-VL-8B-Instruct', revision=None, inference_mode=False, r=64, target_modules='^(model.language_model.*\\.(o_proj|k_proj|q_proj|gate_proj|up_proj|v_proj|down_proj))$', exclude_modules=None, lora_alpha=32, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=[], init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, alora_invocation_tokens=None, use_qalora=False, qalora_group_size=16, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False, target_parameters=None, arrow_config=None, ensure_weight_tying=False, lora_dtype=None, lorap_lr_ratio=None, lorap_emb_lr=1e-06)
[INFO:swift] model: PeftModelForCausalLM(
  (base_model): LoraModel(
    (model): Qwen3VLForConditionalGeneration(
      (model): Qwen3VLModel(
        (visual): Qwen3VLVisionModel(
          (patch_embed): Qwen3VLVisionPatchEmbed(
            (proj): Conv3d(3, 1152, kernel_size=(2, 16, 16), stride=(2, 16, 16))
          )
          (pos_embed): Embedding(2304, 1152)
          (rotary_pos_emb): Qwen3VLVisionRotaryEmbedding()
          (blocks): ModuleList(
            (0-26): 27 x Qwen3VLVisionBlock(
              (norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)
              (norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)
              (attn): Qwen3VLVisionAttention(
                (qkv): Linear(in_features=1152, out_features=3456, bias=True)
                (proj): Linear(in_features=1152, out_features=1152, bias=True)
              )
              (mlp): Qwen3VLVisionMLP(
                (linear_fc1): Linear(in_features=1152, out_features=4304, bias=True)
                (linear_fc2): Linear(in_features=4304, out_features=1152, bias=True)
                (act_fn): GELUTanh()
              )
            )
          )
          (merger): Qwen3VLVisionPatchMerger(
            (norm): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)
            (linear_fc1): Linear(in_features=4608, out_features=4608, bias=True)
            (act_fn): GELU(approximate='none')
            (linear_fc2): Linear(in_features=4608, out_features=4096, bias=True)
          )
          (deepstack_merger_list): ModuleList(
            (0-2): 3 x Qwen3VLVisionPatchMerger(
              (norm): LayerNorm((4608,), eps=1e-06, elementwise_affine=True)
              (linear_fc1): Linear(in_features=4608, out_features=4608, bias=True)
              (act_fn): GELU(approximate='none')
              (linear_fc2): Linear(in_features=4608, out_features=4096, bias=True)
            )
          )
        )
        (language_model): Qwen3VLTextModel(
          (embed_tokens): Embedding(151936, 4096)
          (layers): ModuleList(
            (0-35): 36 x Qwen3VLTextDecoderLayer(
              (self_attn): Qwen3VLTextAttention(
                (q_proj): lora.Linear(
                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=4096, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=64, out_features=4096, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                  (lora_magnitude_vector): ModuleDict()
                )
                (k_proj): lora.Linear(
                  (base_layer): Linear(in_features=4096, out_features=1024, bias=False)
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=4096, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=64, out_features=1024, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                  (lora_magnitude_vector): ModuleDict()
                )
                (v_proj): lora.Linear(
                  (base_layer): Linear(in_features=4096, out_features=1024, bias=False)
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=4096, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=64, out_features=1024, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                  (lora_magnitude_vector): ModuleDict()
                )
                (o_proj): lora.Linear(
                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=4096, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=64, out_features=4096, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                  (lora_magnitude_vector): ModuleDict()
                )
                (q_norm): Qwen3VLTextRMSNorm((128,), eps=1e-06)
                (k_norm): Qwen3VLTextRMSNorm((128,), eps=1e-06)
              )
              (mlp): Qwen3VLTextMLP(
                (gate_proj): lora.Linear(
                  (base_layer): Linear(in_features=4096, out_features=12288, bias=False)
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=4096, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=64, out_features=12288, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                  (lora_magnitude_vector): ModuleDict()
                )
                (up_proj): lora.Linear(
                  (base_layer): Linear(in_features=4096, out_features=12288, bias=False)
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=4096, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=64, out_features=12288, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                  (lora_magnitude_vector): ModuleDict()
                )
                (down_proj): lora.Linear(
                  (base_layer): Linear(in_features=12288, out_features=4096, bias=False)
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=12288, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=64, out_features=4096, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                  (lora_magnitude_vector): ModuleDict()
                )
                (act_fn): SiLUActivation()
              )
              (input_layernorm): Qwen3VLTextRMSNorm((4096,), eps=1e-06)
              (post_attention_layernorm): Qwen3VLTextRMSNorm((4096,), eps=1e-06)
            )
          )
          (norm): Qwen3VLTextRMSNorm((4096,), eps=1e-06)
          (rotary_emb): Qwen3VLTextRotaryEmbedding()
        )
      )
      (lm_head): Linear(in_features=4096, out_features=151936, bias=False)
    )
  )
)
[INFO:swift] model_parameter_info: PeftModelForCausalLM: 8941.7116M Params (174.5879M Trainable [1.9525%]), 0.0001M Buffers.
The model is already on multiple devices. Skipping the move to device specified in `args`.
/usr/local/lib/python3.10/dist-packages/setuptools-68.2.2-py3.10.egg/_distutils_hack/__init__.py:18: UserWarning: Distutils was imported before Setuptools, but importing Setuptools also replaces the `distutils` module in `sys.modules`. This may lead to undesirable behaviors or errors. To avoid these issues, avoid using distutils directly, ensure that setuptools is installed in the traditional way (e.g. not an editable install), and/or make sure that setuptools is always imported before distutils.
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/setuptools-68.2.2-py3.10.egg/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
  warnings.warn("Setuptools is replacing distutils.")
[INFO:swift] use_reentrant: True
[INFO:swift] The logging file will be saved in: /root/paddlejob/workspace/env_run/output/VIDA/sft/output/qwen3_vl_8b_lora_sft_stable/v1-20251228-101733/logging.jsonl
[INFO:swift] Successfully registered post_encode hook: ['PeftModelForCausalLM'].
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 151645, 'bos_token_id': None}.
Train:   0%|                                                                              | 0/144 [00:00<?, ?it/s][INFO:swift] use_logits_to_keep: False
Train:   1%|▍                                                                   | 1/144 [00:52<2:04:46, 52.35s/it]                                                                                                                  Train:   1%|▍                                                                   | 1/144 [00:52<2:04:46, 52.35s/it]Train:   1%|▍                                                                   | 1/144 [00:52<2:04:46, 52.35s/it]Train:   1%|▉                                                                   | 2/144 [01:33<1:48:59, 46.05s/it]Train:   2%|█▍                                                                  | 3/144 [02:15<1:43:30, 44.04s/it]Train:   3%|█▉                                                                  | 4/144 [02:57<1:40:40, 43.14s/it]Train:   3%|██▎                                                                 | 5/144 [03:39<1:38:50, 42.66s/it]                                                                                                                  Train:   3%|██▎                                                                 | 5/144 [03:39<1:38:50, 42.66s/it]Train:   3%|██▎                                                                 | 5/144 [03:39<1:38:50, 42.66s/it]Train:   4%|██▊                                                                 | 6/144 [04:20<1:37:19, 42.31s/it]Train:   5%|███▎                                                                | 7/144 [05:02<1:36:00, 42.05s/it]Train:   6%|███▊                                                                | 8/144 [05:44<1:35:28, 42.12s/it]Train:   6%|████▎                                                               | 9/144 [06:26<1:34:30, 42.00s/it]Train:   7%|████▋                                                              | 10/144 [07:08<1:33:38, 41.93s/it]                                                                                                                  Train:   7%|████▋                                                              | 10/144 [07:08<1:33:38, 41.93s/it]Train:   7%|████▋                                                              | 10/144 [07:08<1:33:38, 41.93s/it]Train:   8%|█████                                                              | 11/144 [07:49<1:32:40, 41.81s/it]Train:   8%|█████▌                                                             | 12/144 [08:31<1:31:58, 41.81s/it]Train:   9%|██████                                                             | 13/144 [09:13<1:31:24, 41.87s/it]Train:  10%|██████▌                                                            | 14/144 [09:55<1:30:30, 41.78s/it]Train:  10%|██████▉                                                            | 15/144 [10:36<1:29:38, 41.69s/it]                                                                                                                  Train:  10%|██████▉                                                            | 15/144 [10:36<1:29:38, 41.69s/it]Train:  10%|██████▉                                                            | 15/144 [10:36<1:29:38, 41.69s/it]Train:  11%|███████▍                                                           | 16/144 [11:18<1:29:16, 41.85s/it]Train:  12%|███████▉                                                           | 17/144 [12:00<1:28:28, 41.80s/it]Train:  12%|████████▍                                                          | 18/144 [12:42<1:27:50, 41.83s/it]Train:  13%|████████▊                                                          | 19/144 [13:24<1:27:02, 41.78s/it]Train:  14%|█████████▎                                                         | 20/144 [14:06<1:26:29, 41.85s/it]                                                                                                                  Train:  14%|█████████▎                                                         | 20/144 [14:06<1:26:29, 41.85s/it]Train:  14%|█████████▎                                                         | 20/144 [14:06<1:26:29, 41.85s/it]Train:  15%|█████████▊                                                         | 21/144 [14:47<1:25:44, 41.82s/it]Train:  15%|██████████▏                                                        | 22/144 [15:29<1:25:06, 41.85s/it]Train:  16%|██████████▋                                                        | 23/144 [16:11<1:24:07, 41.72s/it]Train:  17%|███████████▏                                                       | 24/144 [16:52<1:23:23, 41.70s/it]Train:  17%|███████████▋                                                       | 25/144 [17:33<1:22:24, 41.55s/it]                                                                                                                  Train:  17%|███████████▋                                                       | 25/144 [17:33<1:22:24, 41.55s/it]Train:  17%|███████████▋                                                       | 25/144 [17:33<1:22:24, 41.55s/it]Train:  18%|████████████                                                       | 26/144 [18:15<1:21:46, 41.58s/it]Train:  19%|████████████▌                                                      | 27/144 [18:57<1:21:00, 41.54s/it]Train:  19%|█████████████                                                      | 28/144 [19:38<1:20:20, 41.56s/it]Train:  20%|█████████████▍                                                     | 29/144 [20:20<1:19:40, 41.57s/it]Train:  21%|█████████████▉                                                     | 30/144 [21:02<1:19:14, 41.71s/it]                                                                                                                  Train:  21%|█████████████▉                                                     | 30/144 [21:02<1:19:14, 41.71s/it]Train:  21%|█████████████▉                                                     | 30/144 [21:02<1:19:14, 41.71s/it]Train:  22%|██████████████▍                                                    | 31/144 [21:43<1:18:29, 41.68s/it]Train:  22%|██████████████▉                                                    | 32/144 [22:25<1:17:52, 41.72s/it]Train:  23%|███████████████▎                                                   | 33/144 [23:07<1:17:14, 41.75s/it]Train:  24%|███████████████▊                                                   | 34/144 [23:49<1:16:46, 41.88s/it]Train:  24%|████████████████▎                                                  | 35/144 [24:31<1:15:56, 41.81s/it]                                                                                                                  Train:  24%|████████████████▎                                                  | 35/144 [24:31<1:15:56, 41.81s/it]Train:  24%|████████████████▎                                                  | 35/144 [24:31<1:15:56, 41.81s/it]Train:  25%|████████████████▊                                                  | 36/144 [25:13<1:15:35, 42.00s/it]Train:  26%|█████████████████▏                                                 | 37/144 [25:55<1:14:43, 41.90s/it]Train:  26%|█████████████████▋                                                 | 38/144 [26:37<1:14:03, 41.92s/it]Train:  27%|██████████████████▏                                                | 39/144 [27:18<1:13:02, 41.74s/it]Train:  28%|██████████████████▌                                                | 40/144 [28:00<1:12:22, 41.75s/it]                                                                                                                  Train:  28%|██████████████████▌                                                | 40/144 [28:00<1:12:22, 41.75s/it]Train:  28%|██████████████████▌                                                | 40/144 [28:00<1:12:22, 41.75s/it]Train:  28%|███████████████████                                                | 41/144 [28:42<1:11:42, 41.77s/it]Train:  29%|███████████████████▌                                               | 42/144 [29:24<1:11:02, 41.79s/it]Train:  30%|████████████████████                                               | 43/144 [30:05<1:10:19, 41.77s/it]Train:  31%|████████████████████▍                                              | 44/144 [30:47<1:09:35, 41.75s/it]Train:  31%|████████████████████▉                                              | 45/144 [31:29<1:08:59, 41.81s/it]                                                                                                                  Train:  31%|████████████████████▉                                              | 45/144 [31:29<1:08:59, 41.81s/it]Train:  31%|████████████████████▉                                              | 45/144 [31:29<1:08:59, 41.81s/it]Train:  32%|█████████████████████▍                                             | 46/144 [32:11<1:08:19, 41.83s/it]Train:  33%|█████████████████████▊                                             | 47/144 [32:53<1:07:39, 41.85s/it]Train:  33%|███████████████████████                                              | 48/144 [33:15<57:35, 36.00s/it]{'loss': 4.21146584, 'grad_norm': 1.62078249, 'learning_rate': 2e-05, 'token_acc': 0.37612323, 'epoch': 0.02, 'global_step/max_steps': '1/144', 'percentage': '0.69%', 'elapsed_time': '52s', 'remaining_time': '2h 4m 46s', 'memory(GiB)': 43.4, 'train_speed(iter/s)': 0.0191}
{'loss': 4.05996943, 'grad_norm': 1.67687941, 'learning_rate': 0.0001, 'token_acc': 0.38343604, 'epoch': 0.1, 'global_step/max_steps': '5/144', 'percentage': '3.47%', 'elapsed_time': '3m 39s', 'remaining_time': '1h 41m 34s', 'memory(GiB)': 45.09, 'train_speed(iter/s)': 0.022807}
{'loss': 2.92982273, 'grad_norm': 0.40528631, 'learning_rate': 9.968e-05, 'token_acc': 0.44310405, 'epoch': 0.21, 'global_step/max_steps': '10/144', 'percentage': '6.94%', 'elapsed_time': '7m 8s', 'remaining_time': '1h 35m 37s', 'memory(GiB)': 45.48, 'train_speed(iter/s)': 0.023357}
{'loss': 2.65467472, 'grad_norm': 0.33033505, 'learning_rate': 9.873e-05, 'token_acc': 0.45643739, 'epoch': 0.31, 'global_step/max_steps': '15/144', 'percentage': '10.42%', 'elapsed_time': '10m 36s', 'remaining_time': '1h 31m 14s', 'memory(GiB)': 45.48, 'train_speed(iter/s)': 0.023564}
{'loss': 2.40771961, 'grad_norm': 0.30467492, 'learning_rate': 9.715e-05, 'token_acc': 0.48306281, 'epoch': 0.42, 'global_step/max_steps': '20/144', 'percentage': '13.89%', 'elapsed_time': '14m 6s', 'remaining_time': '1h 27m 25s', 'memory(GiB)': 45.93, 'train_speed(iter/s)': 0.02364}
{'loss': 2.29961185, 'grad_norm': 0.29201266, 'learning_rate': 9.498e-05, 'token_acc': 0.49393579, 'epoch': 0.52, 'global_step/max_steps': '25/144', 'percentage': '17.36%', 'elapsed_time': '17m 33s', 'remaining_time': '1h 23m 36s', 'memory(GiB)': 45.93, 'train_speed(iter/s)': 0.02372}
{'loss': 2.24055786, 'grad_norm': 0.30180225, 'learning_rate': 9.223e-05, 'token_acc': 0.50545929, 'epoch': 0.63, 'global_step/max_steps': '30/144', 'percentage': '20.83%', 'elapsed_time': '21m 2s', 'remaining_time': '1h 19m 56s', 'memory(GiB)': 45.93, 'train_speed(iter/s)': 0.023766}
{'loss': 2.19091644, 'grad_norm': 0.26589519, 'learning_rate': 8.894e-05, 'token_acc': 0.51348804, 'epoch': 0.73, 'global_step/max_steps': '35/144', 'percentage': '24.31%', 'elapsed_time': '24m 31s', 'remaining_time': '1h 16m 22s', 'memory(GiB)': 45.93, 'train_speed(iter/s)': 0.023788}
{'loss': 2.14359913, 'grad_norm': 0.25989109, 'learning_rate': 8.516e-05, 'token_acc': 0.51236411, 'epoch': 0.84, 'global_step/max_steps': '40/144', 'percentage': '27.78%', 'elapsed_time': '28m 0s', 'remaining_time': '1h 12m 49s', 'memory(GiB)': 45.93, 'train_speed(iter/s)': 0.023802}
{'loss': 2.08460445, 'grad_norm': 0.29916972, 'learning_rate': 8.092e-05, 'token_acc': 0.5176986, 'epoch': 0.94, 'global_step/max_steps': '45/144', 'percentage': '31.25%', 'elapsed_time': '31m 29s', 'remaining_time': '1h 9m 17s', 'memory(GiB)': 45.93, 'train_speed(iter/s)': 0.023815}
Train:  34%|██████████████████████▊                                            | 49/144 [34:04<1:03:14, 39.94s/it]Train:  35%|███████████████████████▎                                           | 50/144 [34:46<1:03:18, 40.41s/it]                                                                                                                  Train:  35%|███████████████████████▎                                           | 50/144 [34:46<1:03:18, 40.41s/it]Train:  35%|███████████████████████▎                                           | 50/144 [34:46<1:03:18, 40.41s/it][INFO:swift] Saving model checkpoint to /root/paddlejob/workspace/env_run/output/VIDA/sft/output/qwen3_vl_8b_lora_sft_stable/v1-20251228-101733/checkpoint-50
Train:  35%|███████████████████████▋                                           | 51/144 [35:29<1:03:48, 41.17s/it]Train:  36%|████████████████████████▏                                          | 52/144 [36:10<1:03:15, 41.26s/it]Train:  37%|████████████████████████▋                                          | 53/144 [36:52<1:02:38, 41.30s/it]Train:  38%|█████████████████████████▏                                         | 54/144 [37:33<1:02:09, 41.44s/it]Train:  38%|█████████████████████████▌                                         | 55/144 [38:15<1:01:43, 41.62s/it]                                                                                                                  Train:  38%|█████████████████████████▌                                         | 55/144 [38:15<1:01:43, 41.62s/it]Train:  38%|█████████████████████████▌                                         | 55/144 [38:15<1:01:43, 41.62s/it]Train:  39%|██████████████████████████                                         | 56/144 [38:57<1:00:56, 41.55s/it]Train:  40%|██████████████████████████▌                                        | 57/144 [39:39<1:00:30, 41.73s/it]Train:  40%|███████████████████████████▊                                         | 58/144 [40:21<59:50, 41.74s/it]Train:  41%|████████████████████████████▎                                        | 59/144 [41:03<59:14, 41.82s/it]Train:  42%|████████████████████████████▊                                        | 60/144 [41:45<58:40, 41.91s/it]                                                                                                                  Train:  42%|████████████████████████████▊                                        | 60/144 [41:45<58:40, 41.91s/it]Train:  42%|████████████████████████████▊                                        | 60/144 [41:45<58:40, 41.91s/it]Train:  42%|█████████████████████████████▏                                       | 61/144 [42:27<57:56, 41.88s/it]Train:  43%|█████████████████████████████▋                                       | 62/144 [43:08<57:01, 41.73s/it]Train:  44%|██████████████████████████████▏                                      | 63/144 [43:50<56:15, 41.67s/it]Train:  44%|██████████████████████████████▋                                      | 64/144 [44:31<55:35, 41.69s/it]Train:  45%|███████████████████████████████▏                                     | 65/144 [45:13<54:46, 41.60s/it]                                                                                                                  Train:  45%|███████████████████████████████▏                                     | 65/144 [45:13<54:46, 41.60s/it]Train:  45%|███████████████████████████████▏                                     | 65/144 [45:13<54:46, 41.60s/it]Train:  46%|███████████████████████████████▋                                     | 66/144 [45:55<54:20, 41.80s/it]Train:  47%|████████████████████████████████                                     | 67/144 [46:37<53:35, 41.76s/it]Train:  47%|████████████████████████████████▌                                    | 68/144 [47:18<52:53, 41.76s/it]Train:  48%|█████████████████████████████████                                    | 69/144 [48:00<52:13, 41.79s/it]Train:  49%|█████████████████████████████████▌                                   | 70/144 [48:42<51:27, 41.72s/it]                                                                                                                  Train:  49%|█████████████████████████████████▌                                   | 70/144 [48:42<51:27, 41.72s/it]Train:  49%|█████████████████████████████████▌                                   | 70/144 [48:42<51:27, 41.72s/it]Train:  49%|██████████████████████████████████                                   | 71/144 [49:23<50:43, 41.69s/it]Train:  50%|██████████████████████████████████▌                                  | 72/144 [50:05<49:57, 41.64s/it]Train:  51%|██████████████████████████████████▉                                  | 73/144 [50:47<49:14, 41.62s/it]Train:  51%|███████████████████████████████████▍                                 | 74/144 [51:28<48:33, 41.63s/it]Train:  52%|███████████████████████████████████▉                                 | 75/144 [52:10<47:50, 41.60s/it]                                                                                                                  Train:  52%|███████████████████████████████████▉                                 | 75/144 [52:10<47:50, 41.60s/it]Train:  52%|███████████████████████████████████▉                                 | 75/144 [52:10<47:50, 41.60s/it]Train:  53%|████████████████████████████████████▍                                | 76/144 [52:51<47:07, 41.59s/it]Train:  53%|████████████████████████████████████▉                                | 77/144 [53:33<46:34, 41.71s/it]Train:  54%|█████████████████████████████████████▍                               | 78/144 [54:15<45:51, 41.69s/it]Train:  55%|█████████████████████████████████████▊                               | 79/144 [54:56<45:07, 41.65s/it]Train:  56%|██████████████████████████████████████▎                              | 80/144 [55:38<44:20, 41.58s/it]                                                                                                                  Train:  56%|██████████████████████████████████████▎                              | 80/144 [55:38<44:20, 41.58s/it]Train:  56%|██████████████████████████████████████▎                              | 80/144 [55:38<44:20, 41.58s/it]Train:  56%|██████████████████████████████████████▊                              | 81/144 [56:20<43:46, 41.69s/it]Train:  57%|███████████████████████████████████████▎                             | 82/144 [57:01<42:59, 41.60s/it]Train:  58%|███████████████████████████████████████▊                             | 83/144 [57:43<42:15, 41.57s/it]Train:  58%|████████████████████████████████████████▎                            | 84/144 [58:25<41:40, 41.68s/it]Train:  59%|████████████████████████████████████████▋                            | 85/144 [59:06<40:57, 41.65s/it]                                                                                                                  Train:  59%|████████████████████████████████████████▋                            | 85/144 [59:06<40:57, 41.65s/it]Train:  59%|████████████████████████████████████████▋                            | 85/144 [59:06<40:57, 41.65s/it]Train:  60%|█████████████████████████████████████████▏                           | 86/144 [59:48<40:22, 41.77s/it]Train:  60%|████████████████████████████████████████▍                          | 87/144 [1:00:30<39:45, 41.86s/it]Train:  61%|████████████████████████████████████████▉                          | 88/144 [1:01:12<39:01, 41.81s/it]Train:  62%|█████████████████████████████████████████▍                         | 89/144 [1:01:54<38:24, 41.91s/it]Train:  62%|█████████████████████████████████████████▉                         | 90/144 [1:02:36<37:42, 41.91s/it]                                                                                                                  Train:  62%|█████████████████████████████████████████▉                         | 90/144 [1:02:36<37:42, 41.91s/it]Train:  62%|█████████████████████████████████████████▉                         | 90/144 [1:02:36<37:42, 41.91s/it]Train:  63%|██████████████████████████████████████████▎                        | 91/144 [1:03:18<37:02, 41.93s/it]Train:  64%|██████████████████████████████████████████▊                        | 92/144 [1:04:00<36:18, 41.89s/it]Train:  65%|███████████████████████████████████████████▎                       | 93/144 [1:04:42<35:34, 41.86s/it]Train:  65%|███████████████████████████████████████████▋                       | 94/144 [1:05:24<34:53, 41.88s/it]Train:  66%|████████████████████████████████████████████▏                      | 95/144 [1:06:06<34:18, 42.01s/it]                                                                                                                  Train:  66%|████████████████████████████████████████████▏                      | 95/144 [1:06:06<34:18, 42.01s/it]Train:  66%|████████████████████████████████████████████▏                      | 95/144 [1:06:06<34:18, 42.01s/it]Train:  67%|████████████████████████████████████████████▋                      | 96/144 [1:06:28<28:54, 36.13s/it]{'loss': 2.10789566, 'grad_norm': 0.34107366, 'learning_rate': 7.629e-05, 'token_acc': 0.52233584, 'epoch': 1.04, 'global_step/max_steps': '50/144', 'percentage': '34.72%', 'elapsed_time': '34m 46s', 'remaining_time': '1h 5m 22s', 'memory(GiB)': 45.93, 'train_speed(iter/s)': 0.023966}
{'loss': 1.98148441, 'grad_norm': 0.34867272, 'learning_rate': 7.133e-05, 'token_acc': 0.53792936, 'epoch': 1.15, 'global_step/max_steps': '55/144', 'percentage': '38.19%', 'elapsed_time': '38m 15s', 'remaining_time': '1h 1m 55s', 'memory(GiB)': 45.93, 'train_speed(iter/s)': 0.023955}
{'loss': 1.99084511, 'grad_norm': 0.33084068, 'learning_rate': 6.609e-05, 'token_acc': 0.53791146, 'epoch': 1.25, 'global_step/max_steps': '60/144', 'percentage': '41.67%', 'elapsed_time': '41m 45s', 'remaining_time': '58m 27s', 'memory(GiB)': 45.93, 'train_speed(iter/s)': 0.023949}
{'loss': 1.97321091, 'grad_norm': 0.36938372, 'learning_rate': 6.065e-05, 'token_acc': 0.54592388, 'epoch': 1.36, 'global_step/max_steps': '65/144', 'percentage': '45.14%', 'elapsed_time': '45m 13s', 'remaining_time': '54m 57s', 'memory(GiB)': 45.93, 'train_speed(iter/s)': 0.023957}
{'loss': 1.84673443, 'grad_norm': 0.3743588, 'learning_rate': 5.508e-05, 'token_acc': 0.55215594, 'epoch': 1.46, 'global_step/max_steps': '70/144', 'percentage': '48.61%', 'elapsed_time': '48m 42s', 'remaining_time': '51m 29s', 'memory(GiB)': 45.93, 'train_speed(iter/s)': 0.023954}
{'loss': 1.88313351, 'grad_norm': 0.43268451, 'learning_rate': 4.943e-05, 'token_acc': 0.5513561, 'epoch': 1.57, 'global_step/max_steps': '75/144', 'percentage': '52.08%', 'elapsed_time': '52m 10s', 'remaining_time': '47m 59s', 'memory(GiB)': 45.93, 'train_speed(iter/s)': 0.02396}
{'loss': 1.87583828, 'grad_norm': 0.40868932, 'learning_rate': 4.38e-05, 'token_acc': 0.55736918, 'epoch': 1.67, 'global_step/max_steps': '80/144', 'percentage': '55.56%', 'elapsed_time': '55m 38s', 'remaining_time': '44m 30s', 'memory(GiB)': 46.04, 'train_speed(iter/s)': 0.023964}
{'loss': 1.8930666, 'grad_norm': 0.39272285, 'learning_rate': 3.825e-05, 'token_acc': 0.55163963, 'epoch': 1.78, 'global_step/max_steps': '85/144', 'percentage': '59.03%', 'elapsed_time': '59m 6s', 'remaining_time': '41m 1s', 'memory(GiB)': 46.04, 'train_speed(iter/s)': 0.023966}
{'loss': 1.89988575, 'grad_norm': 0.42298335, 'learning_rate': 3.284e-05, 'token_acc': 0.54367366, 'epoch': 1.88, 'global_step/max_steps': '90/144', 'percentage': '62.50%', 'elapsed_time': '1h 2m 36s', 'remaining_time': '37m 33s', 'memory(GiB)': 46.04, 'train_speed(iter/s)': 0.023958}
{'loss': 1.90349331, 'grad_norm': 0.40592524, 'learning_rate': 2.765e-05, 'token_acc': 0.55272685, 'epoch': 1.99, 'global_step/max_steps': '95/144', 'percentage': '65.97%', 'elapsed_time': '1h 6m 6s', 'remaining_time': '34m 5s', 'memory(GiB)': 46.04, 'train_speed(iter/s)': 0.023951}
Train:  67%|█████████████████████████████████████████████▏                     | 97/144 [1:07:17<31:21, 40.03s/it]Train:  68%|█████████████████████████████████████████████▌                     | 98/144 [1:07:59<31:00, 40.45s/it]Train:  69%|██████████████████████████████████████████████                     | 99/144 [1:08:40<30:33, 40.74s/it]Train:  69%|█████████████████████████████████████████████▊                    | 100/144 [1:09:22<30:11, 41.16s/it]                                                                                                                  Train:  69%|█████████████████████████████████████████████▊                    | 100/144 [1:09:22<30:11, 41.16s/it]Train:  69%|█████████████████████████████████████████████▊                    | 100/144 [1:09:22<30:11, 41.16s/it][INFO:swift] Saving model checkpoint to /root/paddlejob/workspace/env_run/output/VIDA/sft/output/qwen3_vl_8b_lora_sft_stable/v1-20251228-101733/checkpoint-100
Train:  70%|██████████████████████████████████████████████▎                   | 101/144 [1:10:05<29:53, 41.70s/it]Train:  71%|██████████████████████████████████████████████▊                   | 102/144 [1:10:47<29:07, 41.61s/it]Train:  72%|███████████████████████████████████████████████▏                  | 103/144 [1:11:29<28:33, 41.79s/it]Train:  72%|███████████████████████████████████████████████▋                  | 104/144 [1:12:11<27:48, 41.72s/it]Train:  73%|████████████████████████████████████████████████▏                 | 105/144 [1:12:53<27:10, 41.82s/it]                                                                                                                  Train:  73%|████████████████████████████████████████████████▏                 | 105/144 [1:12:53<27:10, 41.82s/it]Train:  73%|████████████████████████████████████████████████▏                 | 105/144 [1:12:53<27:10, 41.82s/it]Train:  74%|████████████████████████████████████████████████▌                 | 106/144 [1:13:34<26:27, 41.78s/it]Train:  74%|█████████████████████████████████████████████████                 | 107/144 [1:14:16<25:46, 41.80s/it]Train:  75%|█████████████████████████████████████████████████▌                | 108/144 [1:14:58<25:05, 41.82s/it]Train:  76%|█████████████████████████████████████████████████▉                | 109/144 [1:15:40<24:21, 41.75s/it]Train:  76%|██████████████████████████████████████████████████▍               | 110/144 [1:16:21<23:37, 41.68s/it]                                                                                                                  Train:  76%|██████████████████████████████████████████████████▍               | 110/144 [1:16:21<23:37, 41.68s/it]Train:  76%|██████████████████████████████████████████████████▍               | 110/144 [1:16:21<23:37, 41.68s/it]Train:  77%|██████████████████████████████████████████████████▉               | 111/144 [1:17:03<22:56, 41.70s/it]Train:  78%|███████████████████████████████████████████████████▎              | 112/144 [1:17:45<22:17, 41.80s/it]Train:  78%|███████████████████████████████████████████████████▊              | 113/144 [1:18:26<21:33, 41.72s/it]Train:  79%|████████████████████████████████████████████████████▎             | 114/144 [1:19:08<20:49, 41.65s/it]Train:  80%|████████████████████████████████████████████████████▋             | 115/144 [1:19:50<20:10, 41.73s/it]                                                                                                                  Train:  80%|████████████████████████████████████████████████████▋             | 115/144 [1:19:50<20:10, 41.73s/it]Train:  80%|████████████████████████████████████████████████████▋             | 115/144 [1:19:50<20:10, 41.73s/it]Train:  81%|█████████████████████████████████████████████████████▏            | 116/144 [1:20:31<19:26, 41.65s/it]Train:  81%|█████████████████████████████████████████████████████▋            | 117/144 [1:21:13<18:44, 41.64s/it]Train:  82%|██████████████████████████████████████████████████████            | 118/144 [1:21:55<18:05, 41.75s/it]Train:  83%|██████████████████████████████████████████████████████▌           | 119/144 [1:22:37<17:23, 41.72s/it]Train:  83%|███████████████████████████████████████████████████████           | 120/144 [1:23:18<16:41, 41.72s/it]                                                                                                                  Train:  83%|███████████████████████████████████████████████████████           | 120/144 [1:23:18<16:41, 41.72s/it]Train:  83%|███████████████████████████████████████████████████████           | 120/144 [1:23:18<16:41, 41.72s/it]Train:  84%|███████████████████████████████████████████████████████▍          | 121/144 [1:24:00<16:01, 41.82s/it]Train:  85%|███████████████████████████████████████████████████████▉          | 122/144 [1:24:42<15:19, 41.79s/it]Train:  85%|████████████████████████████████████████████████████████▍         | 123/144 [1:25:24<14:36, 41.73s/it]Train:  86%|████████████████████████████████████████████████████████▊         | 124/144 [1:26:05<13:53, 41.66s/it]Train:  87%|█████████████████████████████████████████████████████████▎        | 125/144 [1:26:47<13:12, 41.74s/it]                                                                                                                  Train:  87%|█████████████████████████████████████████████████████████▎        | 125/144 [1:26:47<13:12, 41.74s/it]Train:  87%|█████████████████████████████████████████████████████████▎        | 125/144 [1:26:47<13:12, 41.74s/it]Train:  88%|█████████████████████████████████████████████████████████▊        | 126/144 [1:27:29<12:31, 41.77s/it]Train:  88%|██████████████████████████████████████████████████████████▏       | 127/144 [1:28:11<11:49, 41.74s/it]Train:  89%|██████████████████████████████████████████████████████████▋       | 128/144 [1:28:52<11:08, 41.75s/it]Train:  90%|███████████████████████████████████████████████████████████▏      | 129/144 [1:29:34<10:25, 41.72s/it]Train:  90%|███████████████████████████████████████████████████████████▌      | 130/144 [1:30:16<09:43, 41.66s/it]                                                                                                                  Train:  90%|███████████████████████████████████████████████████████████▌      | 130/144 [1:30:16<09:43, 41.66s/it]Train:  90%|███████████████████████████████████████████████████████████▌      | 130/144 [1:30:16<09:43, 41.66s/it]Train:  91%|████████████████████████████████████████████████████████████      | 131/144 [1:30:58<09:02, 41.75s/it]Train:  92%|████████████████████████████████████████████████████████████▌     | 132/144 [1:31:39<08:20, 41.72s/it]Train:  92%|████████████████████████████████████████████████████████████▉     | 133/144 [1:32:21<07:39, 41.79s/it]Train:  93%|█████████████████████████████████████████████████████████████▍    | 134/144 [1:33:03<06:58, 41.84s/it]Train:  94%|█████████████████████████████████████████████████████████████▉    | 135/144 [1:33:45<06:15, 41.77s/it]                                                                                                                  Train:  94%|█████████████████████████████████████████████████████████████▉    | 135/144 [1:33:45<06:15, 41.77s/it]Train:  94%|█████████████████████████████████████████████████████████████▉    | 135/144 [1:33:45<06:15, 41.77s/it]Train:  94%|██████████████████████████████████████████████████████████████▎   | 136/144 [1:34:26<05:33, 41.67s/it]Train:  95%|██████████████████████████████████████████████████████████████▊   | 137/144 [1:35:08<04:52, 41.73s/it]Train:  96%|███████████████████████████████████████████████████████████████▎  | 138/144 [1:35:50<04:10, 41.77s/it]Train:  97%|███████████████████████████████████████████████████████████████▋  | 139/144 [1:36:31<03:28, 41.71s/it]Train:  97%|████████████████████████████████████████████████████████████████▏ | 140/144 [1:37:14<02:47, 41.84s/it]                                                                                                                  Train:  97%|████████████████████████████████████████████████████████████████▏ | 140/144 [1:37:14<02:47, 41.84s/it]Train:  97%|████████████████████████████████████████████████████████████████▏ | 140/144 [1:37:14<02:47, 41.84s/it]Train:  98%|████████████████████████████████████████████████████████████████▋ | 141/144 [1:37:56<02:05, 41.89s/it]Train:  99%|█████████████████████████████████████████████████████████████████ | 142/144 [1:38:37<01:23, 41.71s/it]Train:  99%|█████████████████████████████████████████████████████████████████▌| 143/144 [1:39:19<00:41, 41.82s/it]Train: 100%|██████████████████████████████████████████████████████████████████| 144/144 [1:39:41<00:00, 35.98s/it][INFO:swift] Saving model checkpoint to /root/paddlejob/workspace/env_run/output/VIDA/sft/output/qwen3_vl_8b_lora_sft_stable/v1-20251228-101733/checkpoint-144
                                                                                                                  Train: 100%|██████████████████████████████████████████████████████████████████| 144/144 [1:39:42<00:00, 35.98s/it]Train: 100%|██████████████████████████████████████████████████████████████████| 144/144 [1:39:42<00:00, 35.98s/it]Train: 100%|██████████████████████████████████████████████████████████████████| 144/144 [1:39:42<00:00, 41.55s/it]
[INFO:swift] last_model_checkpoint: /root/paddlejob/workspace/env_run/output/VIDA/sft/output/qwen3_vl_8b_lora_sft_stable/v1-20251228-101733/checkpoint-144
[INFO:swift] best_model_checkpoint: None
[INFO:swift] images_dir: /root/paddlejob/workspace/env_run/output/VIDA/sft/output/qwen3_vl_8b_lora_sft_stable/v1-20251228-101733/images
[INFO:swift] End time of running main: 2025-12-28 11:58:33.820468
{'loss': 1.85035629, 'grad_norm': 0.40060464, 'learning_rate': 2.275e-05, 'token_acc': 0.55432373, 'epoch': 2.08, 'global_step/max_steps': '100/144', 'percentage': '69.44%', 'elapsed_time': '1h 9m 22s', 'remaining_time': '30m 31s', 'memory(GiB)': 46.04, 'train_speed(iter/s)': 0.024021}
{'loss': 1.80181007, 'grad_norm': 0.41302258, 'learning_rate': 1.82e-05, 'token_acc': 0.56258597, 'epoch': 2.19, 'global_step/max_steps': '105/144', 'percentage': '72.92%', 'elapsed_time': '1h 12m 53s', 'remaining_time': '27m 4s', 'memory(GiB)': 46.04, 'train_speed(iter/s)': 0.02401}
{'loss': 1.75850105, 'grad_norm': 0.47038794, 'learning_rate': 1.405e-05, 'token_acc': 0.57098727, 'epoch': 2.29, 'global_step/max_steps': '110/144', 'percentage': '76.39%', 'elapsed_time': '1h 16m 21s', 'remaining_time': '23m 36s', 'memory(GiB)': 46.04, 'train_speed(iter/s)': 0.024009}
{'loss': 1.7484129, 'grad_norm': 0.48159122, 'learning_rate': 1.036e-05, 'token_acc': 0.57394979, 'epoch': 2.4, 'global_step/max_steps': '115/144', 'percentage': '79.86%', 'elapsed_time': '1h 19m 50s', 'remaining_time': '20m 7s', 'memory(GiB)': 46.04, 'train_speed(iter/s)': 0.024007}
{'loss': 1.82388039, 'grad_norm': 0.50324607, 'learning_rate': 7.18e-06, 'token_acc': 0.56330563, 'epoch': 2.5, 'global_step/max_steps': '120/144', 'percentage': '83.33%', 'elapsed_time': '1h 23m 18s', 'remaining_time': '16m 39s', 'memory(GiB)': 46.04, 'train_speed(iter/s)': 0.024006}
{'loss': 1.78058472, 'grad_norm': 0.42938074, 'learning_rate': 4.54e-06, 'token_acc': 0.568259, 'epoch': 2.61, 'global_step/max_steps': '125/144', 'percentage': '86.81%', 'elapsed_time': '1h 26m 47s', 'remaining_time': '13m 11s', 'memory(GiB)': 46.04, 'train_speed(iter/s)': 0.024003}
{'loss': 1.75296974, 'grad_norm': 0.4963139, 'learning_rate': 2.48e-06, 'token_acc': 0.5685899, 'epoch': 2.71, 'global_step/max_steps': '130/144', 'percentage': '90.28%', 'elapsed_time': '1h 30m 16s', 'remaining_time': '9m 43s', 'memory(GiB)': 46.04, 'train_speed(iter/s)': 0.024003}
{'loss': 1.8501133, 'grad_norm': 0.45261937, 'learning_rate': 1.03e-06, 'token_acc': 0.56524733, 'epoch': 2.82, 'global_step/max_steps': '135/144', 'percentage': '93.75%', 'elapsed_time': '1h 33m 45s', 'remaining_time': '6m 15s', 'memory(GiB)': 46.04, 'train_speed(iter/s)': 0.023999}
{'loss': 1.75490055, 'grad_norm': 0.44584307, 'learning_rate': 2e-07, 'token_acc': 0.57036272, 'epoch': 2.92, 'global_step/max_steps': '140/144', 'percentage': '97.22%', 'elapsed_time': '1h 37m 14s', 'remaining_time': '2m 46s', 'memory(GiB)': 46.04, 'train_speed(iter/s)': 0.023997}
{'train_runtime': 5982.6975, 'train_samples_per_second': 1.525, 'train_steps_per_second': 0.024, 'train_loss': 2.08003769, 'token_acc': 0.58367787, 'epoch': 3.0, 'global_step/max_steps': '144/144', 'percentage': '100.00%', 'elapsed_time': '1h 39m 42s', 'remaining_time': '0s', 'memory(GiB)': 46.04, 'train_speed(iter/s)': 0.024069}
[W1228 11:58:35.288832736 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
[W1228 11:58:36.903542503 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
run_lora_single.sh: line 55: 1: command not found
